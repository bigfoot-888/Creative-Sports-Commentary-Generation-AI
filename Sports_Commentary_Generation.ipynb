{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bigfoot-888/Creative-Sports-Commentary-Generation-AI/blob/main/Sports_Commentary_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Object Detection"
      ],
      "metadata": {
        "id": "cWTQdS0Ood_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the YOLOv5 Repo\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "import json\n",
        "import torch\n",
        "import cv2\n",
        "import os\n",
        "from IPython.display import Video, display\n",
        "\n",
        "# Upload the video file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "video_path = list(uploaded.keys())[0]\n",
        "\n",
        "# Loading the model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # Using the small model\n",
        "\n",
        "# YOLOv5 class labels from the COCO dataset\n",
        "class_labels = model.names\n",
        "\n",
        "# JSON for tracking data\n",
        "tracking_data = {}\n",
        "\n",
        "# Processing the video\n",
        "output_path = \"video_tracked.mp4\"\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "# To get the duration of the video for later use\n",
        "total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "duration = total_frames / fps\n",
        "\n",
        "frame_index = 0\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Perform detection using YOLO\n",
        "    results = model(frame)\n",
        "    detections = results.xyxy[0].cpu().numpy()  # [xmin, ymin, xmax, ymax, conf, class]\n",
        "\n",
        "    # Store data for this frame\n",
        "    tracking_data[frame_index] = []\n",
        "\n",
        "    # Annotate the frame\n",
        "    for det in detections:\n",
        "        xmin, ymin, xmax, ymax, confidence, cls = det\n",
        "        xmin, ymin, xmax, ymax = int(xmin), int(ymin), int(xmax), int(ymax)\n",
        "        class_label = class_labels[int(cls)]\n",
        "        confidence = float(confidence)\n",
        "\n",
        "        # Store detection info in JSON format\n",
        "        tracking_data[frame_index].append({\n",
        "            \"class\": class_label,\n",
        "            \"bbox\": [xmin, ymin, xmax, ymax],\n",
        "            \"confidence\": confidence\n",
        "        })\n",
        "\n",
        "        # Draw bounding box and label on the frame\n",
        "        cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (255, 0, 0), 2)\n",
        "        cv2.putText(frame, f'{class_label} {confidence:.2f}', (xmin, ymin - 10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "\n",
        "    # Save the annotated frame\n",
        "    out.write(frame)\n",
        "    frame_index += 1\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "# Save the tracking data to a JSON file\n",
        "json_output_path = \"tracking_data.json\"\n",
        "with open(json_output_path, \"w\") as json_file:\n",
        "    json.dump(tracking_data, json_file, indent=4)\n",
        "\n",
        "# Display the Annotated Video\n",
        "display(Video(output_path, embed=True))\n"
      ],
      "metadata": {
        "id": "ITcCFQGpoxHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing"
      ],
      "metadata": {
        "id": "r2mdVjxvGWOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import math\n",
        "\n",
        "# Load the tracking data file\n",
        "with open(\"tracking_data.json\", \"r\") as json_file:\n",
        "    tracking_data = json.load(json_file)\n",
        "\n",
        "# Function to calculate the distance between two bounding boxes\n",
        "def calculate_distance(bbox1, bbox2):\n",
        "    x1_center = (bbox1[0] + bbox1[2]) / 2\n",
        "    y1_center = (bbox1[1] + bbox1[3]) / 2\n",
        "    x2_center = (bbox2[0] + bbox2[2]) / 2\n",
        "    y2_center = (bbox2[1] + bbox2[3]) / 2\n",
        "    return math.sqrt((x2_center - x1_center) ** 2 + (y2_center - y1_center) ** 2)\n",
        "\n",
        "# Function to calculate movement direction\n",
        "def calculate_direction(start_bbox, end_bbox):\n",
        "    start_center = ((start_bbox[0] + start_bbox[2]) // 2, (start_bbox[1] + start_bbox[3]) // 2)\n",
        "    end_center = ((end_bbox[0] + end_bbox[2]) // 2, (end_bbox[1] + end_bbox[3]) // 2)\n",
        "    dx = end_center[0] - start_center[0]\n",
        "    dy = end_center[1] - start_center[1]\n",
        "\n",
        "    direction = \"\"\n",
        "    if abs(dx) > abs(dy):  # Horizontal movement dominates\n",
        "        direction = \"right\" if dx > 0 else \"left\"\n",
        "    else:  # Vertical movement dominates\n",
        "        direction = \"down\" if dy > 0 else \"up\"\n",
        "    return direction, abs(dx) > 50 or abs(dy) > 50  # Threshold for notable movement\n",
        "\n",
        "# Processed information to be stored here\n",
        "processed_information = []\n",
        "\n",
        "# Processing for each frame\n",
        "frame_keys = sorted([int(key) for key in tracking_data.keys()])\n",
        "for i in range(len(frame_keys) - 1):\n",
        "    frame_i = frame_keys[i]\n",
        "    next_frame_i = frame_keys[i + 1]\n",
        "\n",
        "    current_objects = tracking_data[str(frame_i)]\n",
        "    next_objects = tracking_data[str(next_frame_i)]\n",
        "\n",
        "    # Track movements\n",
        "    for obj_i, obj in enumerate(current_objects):\n",
        "        obj_class = obj[\"class\"]\n",
        "        current_bbox = obj[\"bbox\"]\n",
        "\n",
        "        # Match with the nearest object in the next frame\n",
        "        min_distance = float(\"inf\")\n",
        "        best_match = None\n",
        "\n",
        "        for next_obj in next_objects:\n",
        "            distance = calculate_distance(current_bbox, next_obj[\"bbox\"])\n",
        "            if distance < min_distance:\n",
        "                min_distance = distance\n",
        "                best_match = next_obj\n",
        "\n",
        "        if best_match:\n",
        "            direction, notable = calculate_direction(current_bbox, best_match[\"bbox\"])\n",
        "            if notable:\n",
        "                processed_information.append(f\"Object {obj_class} at frame {frame_i} moved {direction}.\")\n",
        "\n",
        "    # Detect interactions within the same frame\n",
        "    for j in range(len(current_objects)):\n",
        "        obj1 = current_objects[j]\n",
        "        for k in range(j + 1, len(current_objects)):\n",
        "            obj2 = current_objects[k]\n",
        "\n",
        "            # Calculate distance between objects\n",
        "            distance = calculate_distance(obj1[\"bbox\"], obj2[\"bbox\"])\n",
        "            if distance < 50:  # Interaction threshold\n",
        "                processed_information.append(\n",
        "                    f\"Object {obj1['class']} came close to Object {obj2['class']} at frame {frame_i}.\"\n",
        "                )\n",
        "\n",
        "# Output processed information\n",
        "for line in processed_information:\n",
        "    print(line)\n",
        "\n",
        "# Save processed information to a file\n",
        "with open(\"processed_data.txt\", \"w\") as processed_data:\n",
        "    processed_data.write(\"\\n\".join(processed_information))\n"
      ],
      "metadata": {
        "id": "NXnN21pspudU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Models"
      ],
      "metadata": {
        "id": "z6EpUL2fHOJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask the user to choose between Llama and GPTNeo\n",
        "whichLLM = input(\"Write either 'llama' or 'gptneo' as the model of choice: \").strip().lower()\n",
        "\n",
        "if whichLLM == \"llama\":\n",
        "    print(\"You selected Llama.\")\n",
        "elif whichLLM == \"gptneo\":\n",
        "    print(\"You selected GPT-Neo.\")\n",
        "else:\n",
        "    print(\"Invalid choice, defaulting to Llama.\")\n",
        "    whichLLM = \"llama\""
      ],
      "metadata": {
        "id": "WxzgyLShEsa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "if whichLLM == \"gptneo\":\n",
        "    generator = pipeline('text-generation', model='EleutherAI/gpt-neo-125M', device=0)\n"
      ],
      "metadata": {
        "id": "Z8gfWTLJWy3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "if whichLLM == \"llama\":\n",
        "    !pip install huggingface-hub\n",
        "    !huggingface-cli login\n",
        "    !huggingface-cli download --local-dir /content/Llama-3.2-1B-Instruct meta-llama/Llama-3.2-1B-Instruct  --exclude \"original/*\"\n",
        "\n",
        "    # Path to the downloaded model\n",
        "    checkpoint_dir = \"/content/Llama-3.2-1B-Instruct\"\n",
        "\n",
        "    # Load the tokenizer and model from the checkpoint directory\n",
        "    tokenizer = AutoTokenizer.from_pretrained(checkpoint_dir)\n",
        "\n",
        "    llama_model = AutoModelForCausalLM.from_pretrained(checkpoint_dir)\n",
        "\n"
      ],
      "metadata": {
        "id": "MyD-1dlEXLE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Commentary Generation, Audio and Video processing"
      ],
      "metadata": {
        "id": "1tUBNwysHXGG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsLINblq2w_Y"
      },
      "outputs": [],
      "source": [
        "!pip install pydub\n",
        "!pip install gTTS\n",
        "import time\n",
        "import re\n",
        "from gtts import gTTS\n",
        "from pydub import AudioSegment\n",
        "\n",
        "\n",
        "# Generates the audio with gtts\n",
        "def generate_audio(text, filename):\n",
        "    # Generate speech from text\n",
        "    tts = gTTS(text=text, lang='en')\n",
        "\n",
        "    # Save to file\n",
        "    tts.save(filename)\n",
        "\n",
        "def parse_commentary_line_with_frame(line):\n",
        "    # Match lines like the ones in the processed info file\n",
        "    match = re.match(r\"(.+?) frame (\\d+) (.+)?\\.\", line)\n",
        "    if match:\n",
        "        frame_i = int(match.group(2))\n",
        "        timestamp = frame_i / fps  # Convert frame to time in seconds\n",
        "        event = line.strip()\n",
        "        return timestamp, event\n",
        "    return None, None\n",
        "\n",
        "# Reading the processed data\n",
        "with open(\"processed_data.txt\", \"r\") as file:\n",
        "    commentary_lines = file.readlines()\n",
        "\n",
        "# Group commentary by timestamp\n",
        "grouped_commentary = {}\n",
        "for line in commentary_lines:\n",
        "    timestamp, event = parse_commentary_line_with_frame(line)\n",
        "    if timestamp is not None:\n",
        "        if timestamp not in grouped_commentary:\n",
        "            grouped_commentary[timestamp] = []\n",
        "        if len(grouped_commentary[timestamp]) < 4:  # Limit to 3 events per timestamp\n",
        "            grouped_commentary[timestamp].append(event)\n",
        "\n",
        "\n",
        "last_timestamp = None # Last timestamp\n",
        "last_end_time = 0 # End time of last commentary generated\n",
        "playback_time = 0 # Time it takes for a given generated commentary to play\n",
        "\n",
        "# New dictionary to store the selected events to store the name of each audio file\n",
        "selected_grouped_commentary = {}\n",
        "\n",
        "# Generate audio for each group\n",
        "for timestamp, events in grouped_commentary.items():\n",
        "    # If the timestamp exceeds the duration of the video, stop\n",
        "    if timestamp >= duration:\n",
        "        break\n",
        "\n",
        "    # If the current events happen after the previous generated commentary ends + 1 as buffer\n",
        "    if timestamp >= last_end_time + 1:\n",
        "        # Remove the \"at frame X\" part for each\n",
        "        cleaned_events = (re.sub(r\" at frame \\d+\", \"\", event).strip() for event in events)\n",
        "\n",
        "        # Join all the events of the group in one\n",
        "        commentary_text = f\"At {timestamp:.2f} seconds: \" + \", \".join(cleaned_events)\n",
        "\n",
        "        # Create the prompt\n",
        "        input_text = f\"Generate short, creative and thrilling, sports-style commentary for the following event, including the names and actions of the objects involved, and make it sound dramatic and exciting: {commentary_text}\"\n",
        "\n",
        "        # Variables to store the outputs of the LLMs\n",
        "        response = \" \"\n",
        "        trimmed_response = \" \"\n",
        "\n",
        "        if whichLLM == \"llama\":\n",
        "            inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "            input_length = inputs[\"input_ids\"].size(1)  # Number of tokens in the input\n",
        "            max_length = input_length + 12\n",
        "            with torch.no_grad():\n",
        "                outputs = llama_model.generate(\n",
        "                **inputs,\n",
        "                max_length=max_length,  # Limit the length of the output\n",
        "                num_return_sequences=1,\n",
        "                temperature=1,  # Control the randomness of the output\n",
        "                top_p=0.9,  # Nucleus sampling\n",
        "                do_sample=True\n",
        "            )\n",
        "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            if response.startswith(input_text):\n",
        "                trimmed_response = response[len(input_text):].strip()  # Remove prompt from the start\n",
        "            else:\n",
        "                trimmed_response = response  # If prompt isn't at the start, don't trim\n",
        "\n",
        "        elif whichLLM == \"gptneo\":\n",
        "            input_text = f\"Generate short, creative and thrilling, sports-style commentary for the following event: {commentary_text}\"\n",
        "\n",
        "            # In case the model fails to generate anything, retry with a limit on the number of attempts\n",
        "            max_retries = 3\n",
        "            retry_count = 0\n",
        "            while not trimmed_response.strip():\n",
        "                # Too many retries\n",
        "                if retry_count >= max_retries:\n",
        "                    response = \"A lot is happening right now.\"\n",
        "                    break\n",
        "                else:\n",
        "                    response = generator(input_text, max_new_tokens=18, num_return_sequences=1, do_sample=True, temperature=1, top_p=0.9)\n",
        "                    retry_count += 1\n",
        "                    if response and len(response) > 0 and \"generated_text\" in response[0]:\n",
        "                        trimmed_response = response[0][\"generated_text\"][len(input_text):].strip()\n",
        "\n",
        "        print(f\"Generating audio for: {trimmed_response}\")\n",
        "\n",
        "        # How much time there is left of video\n",
        "        remaining_time = duration - timestamp\n",
        "\n",
        "        # Save to a file\n",
        "        filename = f\"commentary_{int(timestamp * 100)}.mp3\"\n",
        "        generate_audio(trimmed_response, filename)\n",
        "\n",
        "        # Calculate how long the audio is, and if it is too long for the remaining time, stop\n",
        "        commentary_audio = AudioSegment.from_file(filename)\n",
        "        playback_time = (len(commentary_audio) / 1000)\n",
        "        if playback_time > remaining_time:\n",
        "            break\n",
        "\n",
        "        last_end_time = timestamp + (len(commentary_audio) / 1000)\n",
        "        selected_grouped_commentary[timestamp] = cleaned_events\n",
        "\n",
        "# Initialize the final audio with silence\n",
        "combined_audio = AudioSegment.silent(duration=0)\n",
        "\n",
        "last_end_time = 0  # Variable to keep track of the last timestamp when audio was added\n",
        "\n",
        "for timestamp, events in selected_grouped_commentary.items():\n",
        "    commentary_file = f\"commentary_{int(timestamp * 100)}.mp3\"\n",
        "\n",
        "    # If just starting\n",
        "    if last_end_time == 0:\n",
        "        # Silence until the timestamp of the first events happens\n",
        "        silence_duration = timestamp * 1000\n",
        "        silence_until_next_clip = AudioSegment.silent(duration=silence_duration)\n",
        "        combined_audio += silence_until_next_clip\n",
        "\n",
        "        # Add the previously saved audio\n",
        "        commentary_audio = AudioSegment.from_file(commentary_file)\n",
        "        combined_audio += commentary_audio\n",
        "    else:\n",
        "        silence_duration = (timestamp - last_end_time) * 1000\n",
        "        if silence_duration > 0:\n",
        "            # Add silence before the commentary if there is a gap between finish and start of next\n",
        "            silence_until_next_clip = AudioSegment.silent(duration=silence_duration)\n",
        "            combined_audio += silence_until_next_clip\n",
        "        commentary_audio = AudioSegment.from_file(commentary_file)\n",
        "        combined_audio += commentary_audio\n",
        "\n",
        "    last_end_time = timestamp + (len(commentary_audio) / 1000)\n",
        "\n",
        "# Export the final combined audio\n",
        "combined_audio.export(\"final_audio.mp3\", format=\"mp3\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the audio and video\n",
        "!apt-get install -y ffmpeg\n",
        "!ffmpeg -y -i video_tracked.mp4 -i final_audio.mp3 -c:v copy -c:a aac -strict experimental final_video_with_audio.mp4\n",
        "\n"
      ],
      "metadata": {
        "id": "3oWViPiYkfkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Licensing & Attributions\n",
        "\n",
        "This Colab notebook uses the following dependencies:\n",
        "\n",
        "- **Llama 3.2** — governed by the Llama 3.2 Community License. Users must obtain the model separately.  \n",
        "  - Attribution: “Built with Llama”  \n",
        "  - License: https://github.com/meta-llama/llama-models/tree/main/models/llama3_2\n",
        "\n",
        "- **gTTS** (Google Text-to-Speech), MIT License  \n",
        "  - Copyright © 2014-2024 Pierre Nicolas Durette  \n",
        "  - See https://github.com/pndurette/gTTS for details\n",
        "\n",
        "- **FFmpeg** — LGPL recommended\n",
        "\n",
        "Other libraries (PyTorch, NumPy, pandas, Matplotlib, Transformers, etc.) are permissively licensed (BSD, MIT, Apache 2.0).  \n",
        "By using this notebook, you agree to comply with all applicable third-party licenses.\n"
      ],
      "metadata": {
        "id": "2kEt0REABxRu"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}